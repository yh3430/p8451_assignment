---
title: "assignment_6"
author: "Yu He"
date: "2023-02-21"
output: html_document
---
= 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(NHANES)
library(tidyverse)
library(rpart)
library(caret)
library(rpart.plot)
library(pROC)
library(e1071)
```

## Section 1, Dataset process and partition
```{r}
hw6_df = NHANES %>% 
  janitor::clean_names() %>% 
  select("age", "race1", "education", "hh_income", "weight", "height", "pulse", "diabetes", "bmi", "phys_active", "smoke100") %>% 
  na.omit()

summary(hw6_df)
# Unbalanced data obersved

# Partition data into training and testing sets.
set.seed(123)
 
training_data <- hw6_df$diabetes %>% createDataPartition(p = 0.7, list = F)
train_data <- hw6_df[training_data, ]
test_data <- hw6_df[-training_data, ]
```

## Section 2, Model fit
# Model 1 classfication Tree model
```{r}
set.seed(123)

#Using 10-fold cross-validation to train model
train_control_tree <- trainControl(method = "cv", number = 10, sampling = "down")

#Create sequence of cp parameters to try 
grid_1 <- expand.grid(cp = seq(0.001, 0.3, by = 0.01))

#Using rpart method to generate regression tree, using all variables in dataset to predict life expectancy
model_fit_1 <- train(diabetes ~ . , data = train_data, method = "rpart", trControl = train_control_tree, tuneGrid = grid_1)

model_fit_1$bestTune
model_fit_1$results

#Can use rpart.plot function to visualize tree
rpart.plot(model_fit_1$finalModel)

#Note you can obtain variable importance on the final model within training data
varImp(model_fit_1)
```

# model 2 fit support vector classifier
```{r}
levels(hw6_df$diabetes) <- c("No", "Yes")
hw6_df$diabetes <- relevel(hw6_df$diabetes, ref = "No")

set.seed(123)

# Specify training control
train_control_svc <- trainControl(method = "cv", number = 10, classProbs = T, sampling = "down")

# Create tuning grid
grid_2 <- expand.grid(C = seq(0.001, 2, length = 30))

# Train model with cross-validation and tuning
# Incorporate different values for cost (C)
model_fit_2 <- train(diabetes ~ ., data = train_data, method = "svmLinear",  trControl = train_control_svc, preProcess = c("center", "scale"), 
                     tuneGrid = grid_2)

model_fit_2$bestTune
model_fit_2$results
#Visualize accuracy versus values of C
plot(model_fit_2)

#Obtain metrics of accuracy from training
confusionMatrix(model_fit_2)

#See information about final model
model_fit_2$finalModel
```

# model 3 fit logistic regression
```{r}
set.seed(123)

# Specify training control
train_control_logistic <- trainControl(method = "cv", number = 10, classProbs = T, sampling = "down")

model_fit_3 <- train(diabetes ~ ., data = train_data, method = "glm",  trControl = train_control_logistic, preProcess = c("center", "scale"), family = "binomial")

summary(model_fit_3)

model_fit_3$results
```

## Section 3, Model evaluation and optimal model selection based on accuracy
```{r}
# Model 1
train_outcome_1 <- predict(model_fit_1, train_data)

model_train_eval_1 = confusionMatrix(train_outcome_1, train_data$diabetes, positive = "No")

# Model 2
train_outcome_2 <- predict(model_fit_2, train_data)

model_train_eval_2 = confusionMatrix(train_outcome_2, train_data$diabetes, positive = "No")

# Model 3
train_outcome_3 <- predict(model_fit_3, train_data)

model_train_eval_3 = confusionMatrix(train_outcome_3, train_data$diabetes, positive = "No")

model_train_eval_1
model_train_eval_2
model_train_eval_3

compare_resamp <- resamples(list(
  classification_tree_cv = model_fit_1,
  svmLinear_cv = model_fit_2,
  logistic_cv = model_fit_3
))

summary(compare_resamp)
dotplot(compare_resamp)

# create table of accuracy and kappa
postResample(train_outcome, train_data$diabetes)
postResample(train_outcome_2, train_data$diabetes)
postResample(train_outcome_3, train_data$diabetes)

# All the evaluation parameters show that the performances of classification tree model and logistic regression model are very close and worse than the support vector machine with a linear classifier model. Based on Accuracy, the lsupport vector machine with a linear classifier model is slightly better. So the final choice of model is the support vector machine with a linear classifier model.
```

## Section 4, evaluation the chosen final model within test dataset
```{r}
# Model 2, the final model - logistic regression model use all features.
test_outcome_2 <- predict(model_fit_2, test_data)

model_eval_2 = confusionMatrix(test_outcome_2, test_data$diabetes, positive = "No")

model_eval_2

# All the evaluation parameters should that all the above three model performed similarly. But the performances of lassso and elasctic nets models are very close and better than the traditional logistic regression model. Based on Accuracy, the lasso model slightly better.
```

## model limitations/considerations of the selected model
```{r}
# limitation/consideration of the support vector machine with a linear classifier model

# Limited performance on non-linearly separable data:
# SVMs with a linear classifier model are only effective when the data is linearly separable. Linearly separable data is data that can be separated into two classes using a straight line or a hyperplane. However, in real-world datasets, it is often the case that the data is not linearly separable. In such cases, SVMs with a linear classifier may not be the best choice for classification tasks. One solution to overcome this limitation is to use non-linear kernels such as polynomial, radial basis function (RBF), or sigmoid kernels, which transform the data into a higher-dimensional space where it is more likely to be linearly separable. However, the use of non-linear kernels can lead to overfitting and increase the complexity of the model.

# High sensitivity to outliers:
# SVMs with a linear classifier model are sensitive to outliers. Outliers are data points that are far from the rest of the data points in a dataset. Outliers can significantly affect the position of the decision boundary, which can lead to poor performance of the model. This is because SVMs try to maximize the margin, which is the distance between the decision boundary and the closest data points. Outliers can significantly increase the margin, which can result in a suboptimal decision boundary. To overcome this limitation, one solution is to remove the outliers before training the model. Alternatively, SVMs with a non-linear kernel can be used, which are less sensitive to outliers.

```



















