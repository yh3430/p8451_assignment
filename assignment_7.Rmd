---
title: "assignment_7"
author: "Yu He"
date: "2023-03-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Step 1, load needed libraries
```{r}
library(tidyverse)
library(randomForest)
library(caret)
library(pROC)
library(rpart.plot)
library(rpart)
library(gbm)
```

## Step 2, data import and cleaning
```{r}
setwd("~/Desktop/CU Spring 2023/P8451 intro to machine learning for EPI/p8451_assignment")

hw7_df = read_csv("mi.data.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  select(-id) %>% 
  mutate(
  across(c("sex", "pulm_adema", "fc", "arr", "diab", "obesity", "asthma", "readmission"), as.factor)
  )

# check data balance. The result shows the data is unbalanced.
summary(hw7_df$readmission)
```

## Step 3, Partition data into training and testing sets

```{r}
set.seed(123)
training_data <- hw7_df$readmission %>% createDataPartition(p = 0.7, list = F)
train_data <- hw7_df[training_data, ]
test_data <- hw7_df[-training_data, ]
```

## Step 4, model fitting
# model 1, bagging 
```{r}
set.seed(123)

#Set our value for mtry hyperparameter (the number of features eligible for selection at each node)

#Remember, in bagging, all predictor features are eligible for selection at each node
mtry_val1 <- expand.grid(.mtry = ncol(train_data)-1)

#Just setting 5-fold cross-validation for fast demonstration.
control_settings <- trainControl(method = "cv", number = 10, sampling = "up")

# model fit

model_fit_1 <- train(readmission ~., data = train_data, method = "rf", metric = "Accuracy", 
                     tuneGrid = mtry_val1, ntree = 200, preProcess = c("center", "scale"), trControl = control_settings)

# visualization and accuracy
model_fit_1$results
model_fit_1$bestTune

varImp(model_fit_1)
plot(varImp(model_fit_1))
confusionMatrix(model_fit_1)
```

# model 2, Elastic Net method
```{r}
# model 2, chooses alpha and lambda via cross-validation using all of the features - Elastic Net method
set.seed(123)

# Construct k-folds in your data
trcontrol = trainControl(method = "cv", number = 10, sampling = "up")

# model 2 fit
model_fit_2 <- train(
  readmission ~., data = train_data, method = "glmnet",
  trControl = trcontrol, preProc=c("center", "scale"), tuneLength = 10, metric = "Accuracy"
  )

# Print the values of alpha and lambda that gave best prediction
model_fit_2$bestTune

# Print all of the options examined
model_fit_2$results

# Model coefficients
coef(model_fit_2$finalModel, model_fit_2$bestTune$lambda)


# visualization and accuracy
varImp(model_fit_2)
plot(varImp(model_fit_2))
confusionMatrix(model_fit_2)
```

# model 3, random forest
```{r}
set.seed(123)
# Setting 5-fold cross-validation for fast demonstration.
control_settings<-trainControl(method = "cv", number = 10, sampling = "up")

# Trying three different values of mtry
mtry_vals_3 <- c(ncol(train_data)-1, sqrt(ncol(train_data)-1), 0.5*ncol(train_data)-1)
mtry_grid_3 <- expand.grid(.mtry = round(mtry_vals_3))

model_fit_3 <- train(readmission ~., data = train_data, method = "rf", metric = "Accuracy", preProc=c("center", "scale"),
                     tuneGrid = mtry_grid_3, trControl = control_settings, ntree=200)

confusionMatrix(model_fit_3)
model_fit_3$results
model_fit_3$bestTune
model_fit_3$finalModel

varImp(model_fit_3)
plot(varImp(model_fit_3))

varImpPlot(model_fit_3$finalModel)
```

# model 4 boosting
```{r}
set.seed(123)

#First example where all hyperparameters are being held constant,  no cross-validation, using bootstrapping default

model_fit_boosting <- train(readmission ~., data = train_data, method = "gbm", distribution = "bernoulli", verbose = F, tuneGrid = data.frame(.n.trees = 1000, .shrinkage = 0.001, .interaction.depth = 1, .n.minobsinnode = 10))

confusionMatrix(model_fit_boosting)

#Second example where I tune hyperparameters
set.seed(123)

#only running a few bootstrapped samples
control.settings<-trainControl(number = 5)
gbm.hyp <- expand.grid(n.trees = (0:10)*100, shrinkage = c(0.01, 0.001), interaction.depth = c(1,3), n.minobsinnode = 10)

model_fit_boosting_2 <- train(readmission  ~., data = train_data, method = "gbm", distribution = "bernoulli", verbose = F, tuneGrid = gbm.hyp, trControl = control.settings)

confusionMatrix(model_fit_boosting_2)
# varImp(model_fit_boosting_2)
```


## Step 5, Model evaluation and optimal model selection based on accuracy
```{r}
# Model 1
train_outcome_1 <- predict(model_fit_1, train_data)

model_train_eval_1 = confusionMatrix(train_outcome_1, train_data$readmission, positive = "0")

# Model 2
train_outcome_2 <- predict(model_fit_2, train_data)

model_train_eval_2 = confusionMatrix(train_outcome_2, train_data$readmission, positive = "0")

# Model 3
train_outcome_3 <- predict(model_fit_3, train_data)

model_train_eval_3 = confusionMatrix(train_outcome_3, train_data$readmission, positive = "0")

# Model boosting
train_outcome_4 <- predict(model_fit_boosting, train_data)

model_train_eval_4 = confusionMatrix(train_outcome_4, train_data$readmission, positive = "0")

model_train_eval_1
model_train_eval_2
model_train_eval_3
model_train_eval_4

compare_resamp <- resamples(list(
  bagging = model_fit_1,
  elastic_net = model_fit_2,
  random_forest = model_fit_3
))

summary(compare_resamp)
dotplot(compare_resamp)

# create table of accuracy and kappa
postResample(train_outcome_1, train_data$readmission)
postResample(train_outcome_2, train_data$readmission)
postResample(train_outcome_3, train_data$readmission)
postResample(train_outcome_4, train_data$readmission)

# All the evaluation parameters show that the performances of random forest model and bagging model are very close and better than the elastic net model. Based on Accuracy, the random forest model is slightly better. So the final choice of model is the random forest model.
```

## Step 6, the performance of final model within test dataset
```{r}
# Model 2, the final model - random forest model.
test_outcome_3 <- predict(model_fit_3, test_data)

model_eval_3 = confusionMatrix(test_outcome_3, test_data$readmission, positive = "0")

model_eval_3

postResample(test_outcome_3, test_data$readmission)

# Find the features that are most important for the prediction
varImp(model_fit_3)
plot(varImp(model_fit_3))

varImpPlot(model_fit_3$finalModel)

# Based on the optimized model (random forest), the most important feature for the model includes:
# wbc: white blood cell count
# age: age at inital MI
# esr: erythrocyte sedimentation rate
# sodium: serum sodium
# alt: liver enzyme
# sbp: systolic blood pressure at intake
```















